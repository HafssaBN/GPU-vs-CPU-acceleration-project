{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile main.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport torch.utils.data as data\n\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torchvision import models\n\nfrom sklearn import decomposition\nfrom sklearn import manifold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom tqdm.notebook import tqdm, trange\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport copy\nimport random\nimport time\nimport os\nimport json\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# Set seed for reproducibility\nSEED = 1234\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\ndef setup(rank, world_size):\n    \n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\ndef cleanup():\n    \n    dist.destroy_process_group()\n\n# Define root directory\nROOT = '.'\n\n# Load CIFAR-10 dataset\ntrain_data = datasets.CIFAR10(root=ROOT, train=True, download=True)\n\n# Calculate mean and std for normalization\nmeans = train_data.data.mean(axis=(0, 1, 2)) / 255\nstds = train_data.data.std(axis=(0, 1, 2)) / 255\nprint(f'Calculated means: {means}')\nprint(f'Calculated stds: {stds}')\n\n# Define data transformations\ntrain_transforms = transforms.Compose([\n    transforms.RandomRotation(5),\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.RandomCrop(32, padding=2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=means, std=stds)\n])\n\ntest_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=means, std=stds)\n])\n\n# Apply transformations to datasets\ntrain_data = datasets.CIFAR10(ROOT, train=True, download=True, transform=train_transforms)\ntest_data = datasets.CIFAR10(ROOT, train=False, download=True, transform=test_transforms)\n\n# Split training data into training and validation sets\nVALID_RATIO = 0.9\nn_train_examples = int(len(train_data) * VALID_RATIO)\nn_valid_examples = len(train_data) - n_train_examples\ntrain_data, valid_data = data.random_split(train_data, [n_train_examples, n_valid_examples])\n\n# Apply test transformations to validation data\nvalid_data = copy.deepcopy(valid_data)\nvalid_data.dataset.transform = test_transforms\n\nprint(f'Number of training examples: {len(train_data)}')\nprint(f'Number of validation examples: {len(valid_data)}')\nprint(f'Number of testing examples: {len(test_data)}')\n\n# Define AlexNet model\nclass AlexNet(nn.Module):\n    def __init__(self, output_dim):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, 3, 2, 1), nn.MaxPool2d(2), nn.ReLU(inplace=True),\n            nn.Conv2d(64, 192, 3, padding=1), nn.MaxPool2d(2), nn.ReLU(inplace=True),\n            nn.Conv2d(192, 384, 3, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, 3, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, 3, padding=1), nn.MaxPool2d(2), nn.ReLU(inplace=True)\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5), nn.Linear(256 * 2 * 2, 4096), nn.ReLU(inplace=True),\n            nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(inplace=True),\n            nn.Linear(4096, output_dim)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        h = x.view(x.shape[0], -1)\n        x = self.classifier(h)\n        return x, h\n\ndef create_model():\n    OUTPUT_DIM = 10\n    model = AlexNet(OUTPUT_DIM)\n    return model\n\n# Initialize model parameters\ndef initialize_parameters(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight.data, nonlinearity='relu')\n        nn.init.constant_(m.bias.data, 0)\n    elif isinstance(m, nn.Linear):\n        nn.init.xavier_normal_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n        nn.init.constant_(m.bias.data, 0)\n\n# Count trainable parameters\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Define distributed data loader for GPU\ndef create_dataloader_gpu(rank, world_size):\n    BATCH_SIZE = 256\n    train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank)\n    train_dataloader = data.DataLoader(train_data, batch_size=BATCH_SIZE, sampler=train_sampler, drop_last=True)\n    val_dataloader = data.DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n    test_dataloader = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n    return train_dataloader, val_dataloader, test_dataloader\n\n# Define data loader for CPU\ndef create_dataloader_cpu():\n    BATCH_SIZE = 256\n    train_dataloader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n    val_dataloader = data.DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n    test_dataloader = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n    return train_dataloader, val_dataloader, test_dataloader\n\n\n# Define training step for GPU\ndef train_gpu(model, iterator, optimizer, criterion, device, batch_size):\n    model.train()\n    epoch_loss = 0\n    epoch_acc = 0\n    total_samples=0\n    start_time = time.monotonic()\n    i = 0\n    for (x, y) in iterator:\n        x = x.to(device)\n        y = y.to(device)\n        total_samples += y.shape[0]\n\n        optimizer.zero_grad()\n        y_hat, _ = model(x)\n        loss = criterion(y_hat, y)\n        acc = calculate_accuracy(y_hat, y)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        if i%50 == 0 and device==0:\n            print(f'train {i} over {len(iterator)}')\n        i += 1\n\n    end_time = time.monotonic()\n    epoch_time = end_time - start_time\n    samples_per_second = total_samples / epoch_time\n    return epoch_loss / len(iterator), epoch_acc / len(iterator), samples_per_second\n\n\n# Define evaluation step for GPU\ndef evaluate_gpu(model, iterator, criterion, device, batch_size):\n    model.eval()\n    epoch_loss = 0\n    epoch_acc = 0\n    total_samples = 0\n    start_time = time.monotonic()\n\n    with torch.no_grad():\n        for (x, y) in iterator:\n            x = x.to(device)\n            y = y.to(device)\n            total_samples += y.shape[0]\n            y_hat, _ = model(x)\n            loss = criterion(y_hat, y)\n            acc = calculate_accuracy(y_hat, y)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n\n    end_time = time.monotonic()\n    epoch_time = end_time - start_time\n    samples_per_second = total_samples / epoch_time\n    return epoch_loss / len(iterator), epoch_acc / len(iterator), samples_per_second\n\n\n# Define training step for CPU\ndef train_cpu(model, iterator, optimizer, criterion, batch_size):\n    model.train()\n    epoch_loss = 0\n    epoch_acc = 0\n    total_samples = 0\n    start_time = time.monotonic()\n\n    for (x, y) in iterator:\n        total_samples += y.shape[0]\n        optimizer.zero_grad()\n        y_hat, _ = model(x)\n        loss = criterion(y_hat, y)\n        acc = calculate_accuracy(y_hat, y)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n\n    end_time = time.monotonic()\n    epoch_time = end_time - start_time\n    samples_per_second = total_samples / epoch_time\n    return epoch_loss / len(iterator), epoch_acc / len(iterator), samples_per_second\n\n\n# Define evaluation step for CPU\ndef evaluate_cpu(model, iterator, criterion, batch_size):\n    model.eval()\n    epoch_loss = 0\n    epoch_acc = 0\n    total_samples = 0\n    start_time = time.monotonic()\n\n    with torch.no_grad():\n        for (x, y) in iterator:\n            total_samples += y.shape[0]\n            y_hat, _ = model(x)\n            loss = criterion(y_hat, y)\n            acc = calculate_accuracy(y_hat, y)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n\n    end_time = time.monotonic()\n    epoch_time = end_time - start_time\n    samples_per_second = total_samples / epoch_time\n    return epoch_loss / len(iterator), epoch_acc / len(iterator), samples_per_second\n\n\ndef calculate_accuracy(y_hat, y, topk=(1,)):\n    maxk = max(topk)\n    batch_size = y.size(0)\n    _, pred = y_hat.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(y.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].flatten().sum(dtype=torch.float32)\n        acc = correct_k / batch_size\n        res.append(acc)\n    return res[0]\n\n# Define epoch time calculator\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\n  \n\n# Main training function\ndef main_train(rank, world_size, root=ROOT, num_epochs=3):\n    BATCH_SIZE = 256\n    setup(rank, world_size)\n    print(f\"Process {rank} initialized.\")\n    \n    # Create model and move to GPU\n    device = torch.device(f'cuda:{rank}')\n    #torch.device(f'cuda:{rank}' if torch.cuda.is_available() and world_size > 1 else \"cpu\")\n    model = create_model().to(device)\n    model.apply(initialize_parameters)\n    #if world_size > 1 and torch.cuda.is_available():\n    ddp_model = DDP(model, device_ids=[rank])\n    #else:\n    #    ddp_model = model\n\n    # Create data loaders\n    #if device.type == 'cpu':\n    #    train_dataloader, val_dataloader, test_dataloader = create_dataloader_cpu()\n    #else:\n    print('dataloaded')\n    train_dataloader, val_dataloader, test_dataloader = create_dataloader_gpu(rank, world_size)\n\n    # Define loss and optimizer\n    LR = 5e-4\n    criterion = nn.CrossEntropyLoss().to(device)\n    optimizer = optim.Adam(ddp_model.parameters(), lr=LR)\n\n    # Training loop\n    best_valid_loss = float('inf')\n\n    # Initialize lists to store metrics\n    training_times = []\n    train_losses = []\n    train_accuracies = []\n    validation_times = []\n    validation_losses = []\n    validation_accuracies = []\n    epoch_times = []\n    train_throughputs = []\n    validation_throughputs = []\n\n\n    for epoch in range(num_epochs):\n        print('epoch ',epoch )\n        \n        start_time = time.monotonic()\n        '''if device.type == 'cpu':\n            train_loss, train_acc, train_throughput = train_cpu(ddp_model, train_dataloader, optimizer, criterion, BATCH_SIZE)\n            valid_loss, valid_acc, valid_throughput = evaluate_cpu(ddp_model, val_dataloader, criterion, BATCH_SIZE)\n        else:'''\n        train_loss, train_acc, train_throughput = train_gpu(ddp_model, train_dataloader, optimizer, criterion, device, BATCH_SIZE)\n        print('finish trained  ',epoch )\n        valid_loss, valid_acc, valid_throughput = evaluate_gpu(ddp_model, val_dataloader, criterion, device, BATCH_SIZE)\n\n\n        #if (device.type != 'cpu' and rank == 0) or device.type == 'cpu':\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(ddp_model.state_dict(), os.path.join(root, 'tut4-model.pt'))\n\n        epoch_mins, epoch_secs = epoch_time(start_time, time.monotonic())\n        end_time = time.monotonic()\n        epoch_time_seconds = end_time - start_time\n\n        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n        \n        training_times.append(epoch_time_seconds - (end_time - start_time)/2) #approximate training time\n        train_losses.append(train_loss)\n        train_accuracies.append(train_acc*100)\n        validation_times.append( (end_time - start_time)/2 )#approximate validation time\n        validation_losses.append(valid_loss)\n        validation_accuracies.append(valid_acc*100)\n        epoch_times.append(epoch_time_seconds)\n        train_throughputs.append(train_throughput)\n        validation_throughputs.append(valid_throughput)\n    \n    # Test the model\n    start_time = time.monotonic()\n    #if (device.type != 'cpu' and rank==0) or device.type == 'cpu':\n        #load model only on rank 0 or if CPU\n    ddp_model.load_state_dict(torch.load(os.path.join(root, 'tut4-model.pt')))\n    \n    '''if world_size > 1 and device.type != 'cpu':\n        # Broadcast model to the rest of the devices if multiple GPUs\n        for param in ddp_model.parameters():\n             dist.broadcast(param.data, src=0)\n\n    if device.type == 'cpu':\n        test_loss, test_acc, test_throughput = evaluate_cpu(ddp_model, test_dataloader, criterion, BATCH_SIZE)\n    else:'''\n    test_loss, test_acc, test_throughput = evaluate_gpu(ddp_model, test_dataloader, criterion, device, BATCH_SIZE)\n\n    end_time = time.monotonic()\n    test_time=end_time-start_time\n    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n    \n    #Generate Confusion Matrix\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for x, y in test_dataloader:\n            if device.type != 'cpu':\n                x = x.to(device)\n                y = y.to(device)\n            y_hat, _ = ddp_model(x)\n            preds = torch.argmax(y_hat, dim=1).cpu().numpy()\n            labels = y.cpu().numpy()\n\n            all_preds.extend(preds)\n            all_labels.extend(labels)\n            \n    cm = confusion_matrix(all_labels, all_preds)\n    \n    # Store results in a dictionary\n    results = {\n        f\"model_vgg11_epochs_{num_epochs}_{world_size}_{device.type}_rank_{rank}\": {\n            \"world_size\": world_size,\n            \"rank\": rank,\n            \"device\": device.type,\n            \"training_times\": training_times,\n            \"train_losses\": train_losses,\n            \"train_accurcy\": train_accuracies,\n            \"validation_times\": validation_times,\n            \"validation_losses\": validation_losses,\n            \"validation_accurcy\": validation_accuracies,\n             \"test_time\": test_time,\n            \"test_loss\": test_loss,\n            \"test_acc\": test_acc*100,\n            \"epoch_times\": epoch_times,\n             \"train_throughputs\": train_throughputs,\n            \"validation_throughputs\": validation_throughputs,\n             \"test_throughput\": test_throughput,\n            \"confusion_matrix\": cm.tolist()\n        }\n    }\n        \n    \n    print(f'Process {(rank )} finished training on {device.type}.')\n\n     # Save results as a JSON file (both ranks)\n    results_file = os.path.join(root, 'project3_2gpus.json')\n    \n    # Load existing data if the file exists\n    try:\n        with open(results_file, 'r') as f:\n            all_results = json.load(f)\n    except (FileNotFoundError, json.JSONDecodeError):\n        all_results = {}\n\n    # Update the results with the current rank's data\n    all_results.update(results)\n        \n    with open(results_file, 'w') as f:\n        json.dump(all_results, f, indent=4)\n        print(f\"Results saved to {results_file}\")\n    \n    # if world_size > 1 and device.type != 'cpu':\n    dist.destroy_process_group()\n\n\n# Main execution\nif __name__ == \"__main__\":\n    def main():\n        world_size = torch.cuda.device_count()\n        print(f'Total number of devices detected: {world_size}')\n        # world_size -= 1  # delete this if you want to use 2 gpus keep it for 1 gpu \n        if world_size >= 1:\n            if world_size > 1:\n                mp.spawn(main_train, args=(world_size,), nprocs=world_size, join=True)\n            else:\n                main_train(rank=0, world_size=1)\n        else:\n            print('No GPUs found. Running on CPU only.')\n            \n        \n            \n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T07:13:01.240056Z","iopub.execute_input":"2025-01-10T07:13:01.240399Z","iopub.status.idle":"2025-01-10T07:13:01.251438Z","shell.execute_reply.started":"2025-01-10T07:13:01.240365Z","shell.execute_reply":"2025-01-10T07:13:01.250576Z"}},"outputs":[{"name":"stdout","text":"Writing main.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!python main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T07:13:01.252629Z","iopub.execute_input":"2025-01-10T07:13:01.252919Z","iopub.status.idle":"2025-01-10T07:14:21.069608Z","shell.execute_reply.started":"2025-01-10T07:13:01.252872Z","shell.execute_reply":"2025-01-10T07:14:21.068656Z"}},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n100%|███████████████████████| 170498071/170498071 [00:02<00:00, 58274587.76it/s]\nExtracting ./cifar-10-python.tar.gz to .\nCalculated means: [0.49139968 0.48215841 0.44653091]\nCalculated stds: [0.24703223 0.24348513 0.26158784]\nFiles already downloaded and verified\nFiles already downloaded and verified\nNumber of training examples: 45000\nNumber of validation examples: 5000\nNumber of testing examples: 10000\nTotal number of devices detected: 2\nFiles already downloaded and verified\nFiles already downloaded and verified\nCalculated means: [0.49139968 0.48215841 0.44653091]\nCalculated stds: [0.24703223 0.24348513 0.26158784]\nCalculated means: [0.49139968 0.48215841 0.44653091]\nCalculated stds: [0.24703223 0.24348513 0.26158784]\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nNumber of training examples: 45000\nNumber of validation examples: 5000\nNumber of testing examples: 10000\n[W110 07:13:22.520234971 socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:12355 (errno: 99 - Cannot assign requested address).\nNumber of training examples: 45000\nNumber of validation examples: 5000\nNumber of testing examples: 10000\nProcess 1 initialized.\nProcess 0 initialized.\ndataloaded\ndataloaded\nepoch  0\nepoch  0\nfinish trained   0\nfinish trained   0\nEpoch: 01 | Epoch Time: 0m 16s\n\tTrain Loss: 2.689 | Train Acc: 16.87%\n\t Val. Loss: 2.046 |  Val. Acc: 23.09%\nepoch  1\nEpoch: 01 | Epoch Time: 0m 16s\n\tTrain Loss: 2.700 | Train Acc: 16.31%\n\t Val. Loss: 2.046 |  Val. Acc: 23.09%\nepoch  1\nfinish trained   1\nfinish trained   1\nEpoch: 02 | Epoch Time: 0m 16s\n\tTrain Loss: 1.733 | Train Acc: 35.46%\n\t Val. Loss: 1.578 |  Val. Acc: 42.19%\nepoch  2\nEpoch: 02 | Epoch Time: 0m 16s\n\tTrain Loss: 1.728 | Train Acc: 35.43%\n\t Val. Loss: 1.578 |  Val. Acc: 42.19%\nepoch  2\nfinish trained   2\nfinish trained   2\nEpoch: 03 | Epoch Time: 0m 15s\n\tTrain Loss: 1.509 | Train Acc: 44.92%\n\t Val. Loss: 1.525 |  Val. Acc: 45.79%\n/kaggle/working/main.py:365: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ddp_model.load_state_dict(torch.load(os.path.join(root, 'tut4-model.pt')))\nEpoch: 03 | Epoch Time: 0m 15s\n\tTrain Loss: 1.514 | Train Acc: 44.50%\n\t Val. Loss: 1.525 |  Val. Acc: 45.79%\n/kaggle/working/main.py:365: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ddp_model.load_state_dict(torch.load(os.path.join(root, 'tut4-model.pt')))\nTest Loss: 1.505 | Test Acc: 46.34%\nTest Loss: 1.505 | Test Acc: 46.34%\nProcess 1 finished training on cuda.\nResults saved to ./project3_2gpus.json\nProcess 0 finished training on cuda.\nResults saved to ./project3_2gpus.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}