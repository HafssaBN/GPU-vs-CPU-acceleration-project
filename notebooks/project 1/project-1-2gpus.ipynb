{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile main.py\nfrom torch.utils import data\nimport os\nimport numpy as np\nimport torch\n\nimport torchvision\nfrom torchvision import datasets, models, transforms\n\nfrom tqdm import tqdm\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\nimport copy\nimport random\nimport time\nimport os\nimport json\n\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\nSEED = 1234\nROOT = \".\"\nMODEL_NAME = \"VGG16\"\nSENARIO = \"2GPU\"\nEPOCHS = 3\nBATCH_SIZE = 64\n\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n\"\"\"# 2. Initialize the DDP Environment\"\"\"\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'  # Change this to the master node's IP address if using multiple machines\n    os.environ['MASTER_PORT'] = '12345'  # Pick a free port on the master node\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\"\"\"# 3. Define a Model.\"\"\"\n\n\n# define the CNN architecture\nvgg16 = models.vgg16(pretrained=True)\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef create_model():\n\n    for param in vgg16.features.parameters():\n        param.requires_grad = False\n                     \n    n_inputs = vgg16.classifier[6].in_features\n    last_layer = nn.Linear(n_inputs, 10)\n    vgg16.classifier[6] = last_layer\n    print(f'The model has {count_parameters(vgg16):,} trainable parameters')\n    \n    model = vgg16\n    return model\n\n\"\"\"# 4. Create a Dummy Dataset\"\"\"\n\ndef create_dataloader(rank, world_size, batch_size=BATCH_SIZE, root = ROOT, max_length = 256):\n    data_transform = transforms.Compose([transforms.RandomResizedCrop(224), \n                                      transforms.ToTensor()])\n    ## load the data with\n    outdir = f\"{root}/data\"\n    if rank == 0 and not os.path.exists(outdir):\n        train_data = datasets.CIFAR10(outdir, train=True,\n                                      download=True, transform=data_transform)\n        test_data = datasets.CIFAR10(outdir, train=False,\n                                    download=True, transform=data_transform)\n\n    dist.barrier()  # Ensure all processes wait for the dataset to be downloaded\n\n    train_data = datasets.CIFAR10(outdir, train=True,\n                                      download=True, transform=data_transform)\n    test_data = datasets.CIFAR10(outdir, train=False,\n                                    download=True, transform=data_transform)\n    ## create the validation split\n    VALID_RATIO = 0.9\n\n    n_train_examples = int(len(train_data) * VALID_RATIO)\n    n_valid_examples = len(train_data) - n_train_examples\n    train_data, valid_data = data.random_split(train_data,\n                                           [n_train_examples, n_valid_examples])\n\n    if rank == 0:\n        print(f'Number of training examples: {len(train_data)}')\n        print(f'Number of validation examples: {len(valid_data)}')\n        print(f'Number of testing examples: {len(test_data)}')\n\n\n    ## Creating Data Loaders\n\n    train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank, shuffle=True)\n    val_sampler = DistributedSampler(valid_data, num_replicas=world_size, rank=rank)\n\n    train_dataloader = data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, pin_memory=True) #use num_workers > 0 for better performance\n    val_dataloader = data.DataLoader(valid_data, batch_size=batch_size, sampler=val_sampler, pin_memory=True) #use num_workers > 0 for better performance\n    test_dataloader = data.DataLoader(test_data, batch_size=batch_size, shuffle=False, pin_memory=True) #no sampling for test dataset\n    return train_dataloader, val_dataloader, test_dataloader\n\n\"\"\"# 5. Implement the Training Loop\n\n## a. Help function\n\"\"\"\n\nRESULTS_FILE = f\"{ROOT}/{MODEL_NAME}_{EPOCHS}epochs_{SENARIO}.json\"\n\ndef log_results(scenario, results):\n    \"\"\"\n    Save results to a JSON file for comparison across scenarios.\n    \"\"\"\n    if os.path.exists(RESULTS_FILE):\n        with open(RESULTS_FILE, 'r') as f:\n            all_results = json.load(f)\n    else:\n        all_results = {}\n\n    all_results[scenario] = results\n\n    with open(RESULTS_FILE, 'w') as f:\n        json.dump(all_results, f, indent=4)\n\ndef calculate_accuracy(y_pred, y):\n    top_pred = y_pred.argmax(1, keepdim=True)\n    correct = top_pred.eq(y.view_as(top_pred)).sum()\n    acc = correct.float() / y.shape[0]\n    return acc\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\"\"\"## b. train function\"\"\"\ndef train(model, iterator, optimizer, criterion, rank):\n\n    epoch_loss = 0\n    epoch_acc = 0\n\n    model.train()\n    i=0\n    for (x, y) in tqdm(iterator, desc=f\"Training on the rank {rank}...\", leave=False):\n\n        x = x.to(rank)\n        y = y.to(rank)\n\n        optimizer.zero_grad()\n\n        y_pred = model(x)\n\n        loss = criterion(y_pred, y)\n\n        acc = calculate_accuracy(y_pred, y)\n\n        loss.backward()\n\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        if i % 50 == 0 and rank == 0 :\n            print(f\"- On Training: {i} was passed over  {len(iterator)}\")\n        i+=1\n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\n\n\"\"\"## c. Validation function\"\"\"\ndef evaluate(model, iterator, criterion, rank, mode = \"Evaluating\"):\n\n    epoch_loss = 0\n    epoch_acc = 0\n\n    model.eval()\n    i=0\n    with torch.no_grad():\n\n        for (x, y) in tqdm(iterator, desc=f\"{mode} on the rank {rank} ...\", leave=False):\n\n            x = x.to(rank)\n            y = y.to(rank)\n\n            y_pred = model(x)\n\n            loss = criterion(y_pred, y)\n\n            acc = calculate_accuracy(y_pred, y)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n\n            if i % 50 == 0 and rank == 0:\n                print(f\"- On {mode}: {i} was passed over  {len(iterator)}\")\n            i+=1\n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n\n\n\"\"\"## d. Main loop\"\"\"\n\noutdir = f'{ROOT}/model/'\nif not os.path.exists(outdir):\n    os.makedirs(outdir)\n\ndef main_train(rank, world_size, root = outdir, num_epochs = EPOCHS, model_name = MODEL_NAME):\n    ## a. Set up the distributed process groups\n    setup(rank, world_size)\n    print(f\"Process {rank} initialized.\")\n\n    # setup mp_model and devices for this process\n\n\n    ## b. Create Model, DataLoader\n    train_dataloader, val_dataloader, test_dataloader = create_dataloader(rank, world_size)\n    model = create_model().to(rank)\n\n    ## c. Wrap the model with DistributedDataParallel\n    ddp_model = DDP(model, device_ids=[rank])\n\n    ## d. Loss and Optimizer\n    #LR = 5e-4\n    criterion = nn.CrossEntropyLoss().to(rank) # Move loss to GPU\n    optimizer = optim.Adam(ddp_model.parameters(), lr=0.01)\n\n    ## e. Training Loop\n    best_valid_loss = float('inf')\n    training_times = []\n    train_losses = []\n    train_accurcy = []\n    validation_times = []\n    validation_losses = []\n    validation_accurcy = []\n\n    epoch_times = []\n\n    for epoch in range(num_epochs):\n        start_epoch_time = time.monotonic()\n        start_time = time.monotonic()\n\n        train_loss, train_acc = train(ddp_model, train_dataloader, optimizer, criterion, rank)\n        train_time = time.monotonic() - start_time\n        training_times.append(train_time)\n        train_losses.append(train_loss)\n        train_accurcy.append(train_acc)\n\n        start_time = time.monotonic()\n        valid_loss, valid_acc = evaluate(ddp_model, val_dataloader, criterion, rank)\n        val_time = time.monotonic() - start_time\n        validation_times.append(val_time)\n        validation_losses.append(valid_loss)\n        validation_accurcy.append(valid_acc)\n\n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(ddp_model.state_dict(), f'{root}mlp-model.pt')\n\n        end_time = time.monotonic()\n        e_time = end_time - start_epoch_time\n        epoch_times.append(e_time)\n        epoch_mins, epoch_secs = epoch_time(start_epoch_time, end_time)\n\n        print(f'--------------|     On process {rank}      |----------------')\n        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n\n    ## f. test after train\n    ddp_model.load_state_dict(torch.load(f'{root}mlp-model.pt'))\n    start_time = time.monotonic()\n    test_loss, test_acc = evaluate(ddp_model, test_dataloader, criterion, rank, mode = \"Testing\")\n    test_time = time.monotonic() - start_time\n    print(f'Test results on process {rank}: Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n\n    # Log results\n    results = {\n        \"world_size\": world_size,\n        \"rank\": rank,\n        \"training_times\": training_times,\n        \"train_losses\": train_losses,\n        \"train_accurcy\": train_accurcy,\n        \"validation_times\": validation_times,\n        \"validation_losses\": validation_losses,\n        \"validation_accurcy\": validation_accurcy,\n        \"test_time\": test_time,\n        \"test_loss\": test_loss,\n        \"test_acc\": test_acc,\n        \"epoch_times\": epoch_times\n     }\n\n    scenario = f\"model_{model_name}_epochs_{num_epochs}_{world_size}_GPUs_rank_{rank}\"\n    log_results(scenario, results)\n    dist.barrier()\n\n    cleanup()\n    print(f'Process {rank} finished training.')\n\n\"\"\"# 6. Main Execution\"\"\"\nif __name__ == \"__main__\":\n\n    def main():\n        world_size = torch.cuda.device_count()\n        print(f'Total number of devices detected: {world_size}')\n\n        if world_size >= 1:\n            #start the training process on all available GPUs\n\n            if world_size > 1:\n                #start the training process on all available GPUs\n\n                mp.spawn(\n                    main_train,\n                    args=(world_size,),\n                    nprocs=world_size,\n                    join=True\n                )\n            else:\n                #run training on single GPU\n                main_train(rank=0, world_size=1)\n\n        else:\n            print('no GPUs found. Please make sure you have configured CUDA correctly')\n\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:50:02.465331Z","iopub.execute_input":"2025-01-10T10:50:02.465552Z","iopub.status.idle":"2025-01-10T10:50:02.472439Z","shell.execute_reply.started":"2025-01-10T10:50:02.465532Z","shell.execute_reply":"2025-01-10T10:50:02.471772Z"}},"outputs":[{"name":"stdout","text":"Writing main.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!python main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:32:46.145415Z","iopub.execute_input":"2025-01-10T10:32:46.145715Z","execution_failed":"2025-01-10T10:49:27.933Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nTotal number of devices detected: 2\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nProcess 1 initialized.\nProcess 0 initialized.\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nNumber of training examples: 45000\nNumber of validation examples: 5000\nNumber of testing examples: 10000\nThe model has 119,586,826 trainable parameters\nThe model has 119,586,826 trainable parameters\nTraining on the rank 1...:   0%|                | 1/352 [00:00<05:12,  1.12it/s]- On Training: 0 was passed over  352\nTraining on the rank 0...:  14%|██▏            | 50/352 [00:29<03:00,  1.67it/s]- On Training: 50 was passed over  352\nTraining on the rank 1...:  29%|████          | 101/352 [01:00<02:26,  1.71it/s]- On Training: 100 was passed over  352\nTraining on the rank 0...:  43%|█████▉        | 150/352 [01:29<02:00,  1.67it/s]- On Training: 150 was passed over  352\nTraining on the rank 0...:  57%|███████▉      | 200/352 [01:58<01:33,  1.63it/s]- On Training: 200 was passed over  352\nTraining on the rank 0...:  71%|█████████▉    | 250/352 [02:27<00:59,  1.72it/s]- On Training: 250 was passed over  352\nTraining on the rank 0...:  85%|███████████▉  | 300/352 [02:56<00:30,  1.72it/s]- On Training: 300 was passed over  352\nTraining on the rank 0...:  99%|█████████████▉| 350/352 [03:26<00:01,  1.71it/s]- On Training: 350 was passed over  352\nEvaluating on the rank 1 ...:   0%|                      | 0/40 [00:00<?, ?it/s]- On Evaluating: 0 was passed over  40\n--------------|     On process 0      |----------------                         \nEpoch: 01 | Epoch Time: 3m 45s\n\tTrain Loss: 4.811 | Train Acc: 17.00%\n\t Val. Loss: 1.964 |  Val. Acc: 22.93%\nTraining on the rank 0...:   0%|                        | 0/352 [00:00<?, ?it/s]--------------|     On process 1      |----------------\nEpoch: 01 | Epoch Time: 3m 46s\n\tTrain Loss: 5.053 | Train Acc: 16.62%\n\t Val. Loss: 1.960 |  Val. Acc: 23.59%\nTraining on the rank 1...:   0%|                | 1/352 [00:00<03:23,  1.73it/s]- On Training: 0 was passed over  352\nTraining on the rank 1...:  14%|██▏            | 51/352 [00:30<03:01,  1.66it/s]- On Training: 50 was passed over  352\nTraining on the rank 0...:  28%|███▉          | 100/352 [01:00<02:25,  1.73it/s]- On Training: 100 was passed over  352\nTraining on the rank 1...:  43%|██████        | 151/352 [01:29<01:59,  1.68it/s]- On Training: 150 was passed over  352\nTraining on the rank 1...:  57%|███████▉      | 201/352 [01:59<01:32,  1.63it/s]- On Training: 200 was passed over  352\nTraining on the rank 0...:  71%|█████████▉    | 250/352 [02:30<01:02,  1.64it/s]- On Training: 250 was passed over  352\nTraining on the rank 0...:  85%|███████████▉  | 300/352 [03:00<00:32,  1.60it/s]- On Training: 300 was passed over  352\nTraining on the rank 0...:  99%|█████████████▉| 350/352 [03:29<00:01,  1.69it/s]- On Training: 350 was passed over  352\nEvaluating on the rank 1 ...:   0%|                      | 0/40 [00:00<?, ?it/s]- On Evaluating: 0 was passed over  40\n--------------|     On process 1      |----------------                         \nEpoch: 02 | Epoch Time: 3m 48s\n\tTrain Loss: 2.260 | Train Acc: 16.88%\n\t Val. Loss: 1.951 |  Val. Acc: 24.14%\n--------------|     On process 0      |----------------\nEpoch: 02 | Epoch Time: 3m 49s\n\tTrain Loss: 2.236 | Train Acc: 17.13%\n\t Val. Loss: 1.962 |  Val. Acc: 22.30%\nTraining on the rank 0...:   0%|                        | 0/352 [00:00<?, ?it/s]- On Training: 0 was passed over  352\nTraining on the rank 0...:  14%|██▏            | 50/352 [00:29<02:55,  1.72it/s]- On Training: 50 was passed over  352\nTraining on the rank 0...:  28%|███▉          | 100/352 [00:59<02:26,  1.72it/s]- On Training: 100 was passed over  352\nTraining on the rank 0...:  43%|█████▉        | 150/352 [01:28<01:56,  1.73it/s]- On Training: 150 was passed over  352\nTraining on the rank 1...:  57%|███████▉      | 201/352 [01:58<01:28,  1.71it/s]- On Training: 200 was passed over  352\nTraining on the rank 0...:  71%|█████████▉    | 250/352 [02:27<01:00,  1.70it/s]- On Training: 250 was passed over  352\nTraining on the rank 0...:  85%|███████████▉  | 300/352 [02:56<00:30,  1.69it/s]- On Training: 300 was passed over  352\nTraining on the rank 0...:  99%|█████████████▉| 350/352 [03:25<00:01,  1.58it/s]- On Training: 350 was passed over  352\nEvaluating on the rank 0 ...:   0%|                      | 0/40 [00:00<?, ?it/s]- On Evaluating: 0 was passed over  40\n--------------|     On process 0      |----------------                         \nEpoch: 03 | Epoch Time: 3m 45s\n\tTrain Loss: 2.127 | Train Acc: 18.68%\n\t Val. Loss: 1.955 |  Val. Acc: 24.10%\nTraining on the rank 0...:   0%|                        | 0/352 [00:00<?, ?it/s]--------------|     On process 1      |----------------\nEpoch: 03 | Epoch Time: 3m 46s\n\tTrain Loss: 2.125 | Train Acc: 18.91%\n\t Val. Loss: 1.932 |  Val. Acc: 26.09%\nTraining on the rank 1...:   0%|                | 1/352 [00:00<03:22,  1.73it/s]- On Training: 0 was passed over  352\nTraining on the rank 0...:  14%|██▏            | 50/352 [00:30<03:08,  1.61it/s]- On Training: 50 was passed over  352\nTraining on the rank 0...:  28%|███▉          | 100/352 [00:59<02:26,  1.72it/s]- On Training: 100 was passed over  352\nTraining on the rank 1...:  43%|█████▉        | 150/352 [01:28<01:58,  1.71it/s]- On Training: 150 was passed over  352\nTraining on the rank 0...:  57%|███████▉      | 200/352 [01:58<01:28,  1.72it/s]- On Training: 200 was passed over  352\nTraining on the rank 0...:  71%|█████████▉    | 250/352 [02:28<00:59,  1.71it/s]- On Training: 250 was passed over  352\nTraining on the rank 1...:  86%|███████████▉  | 301/352 [02:57<00:30,  1.70it/s]- On Training: 300 was passed over  352\nTraining on the rank 1...: 100%|█████████████▉| 351/352 [03:27<00:00,  1.67it/s]- On Training: 350 was passed over  352\nEvaluating on the rank 0 ...:   0%|                      | 0/40 [00:00<?, ?it/s]- On Evaluating: 0 was passed over  40\n--------------|     On process 0      |----------------                         \nEpoch: 04 | Epoch Time: 3m 45s\n\tTrain Loss: 2.356 | Train Acc: 14.44%\n\t Val. Loss: 2.095 |  Val. Acc: 18.44%\n--------------|     On process 1      |----------------                         \nEpoch: 04 | Epoch Time: 3m 45s\n\tTrain Loss: 2.358 | Train Acc: 14.20%\n\t Val. Loss: 2.077 |  Val. Acc: 20.35%\nTraining on the rank 1...:   0%|                        | 0/352 [00:00<?, ?it/s]- On Training: 0 was passed over  352\nTraining on the rank 0...:  14%|██▏            | 50/352 [00:30<02:57,  1.70it/s]- On Training: 50 was passed over  352\nTraining on the rank 0...:  28%|███▉          | 100/352 [01:00<02:27,  1.71it/s]- On Training: 100 was passed over  352\nTraining on the rank 0...:  40%|█████▌        | 140/352 [01:24<02:04,  1.71it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!python main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:50:05.733299Z","iopub.execute_input":"2025-01-10T10:50:05.733635Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|█████████████████████████████████████████| 528M/528M [00:02<00:00, 227MB/s]\nTotal number of devices detected: 2\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n[W110 10:50:21.303943620 socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:12345 (errno: 99 - Cannot assign requested address).\nProcess 1 initialized.\nProcess 0 initialized.\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n100%|██████████████████████| 170498071/170498071 [00:01<00:00, 105743386.71it/s]\nExtracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nNumber of training examples: 45000\nNumber of validation examples: 5000\nNumber of testing examples: 10000\nThe model has 119,586,826 trainable parameters\nThe model has 119,586,826 trainable parameters\nTraining on the rank 1...:   0%|                | 1/352 [00:02<13:19,  2.28s/it]- On Training: 0 was passed over  352\nTraining on the rank 1...:  14%|██▏            | 50/352 [00:29<02:45,  1.83it/s]- On Training: 50 was passed over  352\nTraining on the rank 1...:  28%|████▏          | 98/352 [00:55<02:17,  1.84it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}