{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchtext==0.17.0 torch==2.2.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T04:57:56.851109Z","iopub.execute_input":"2025-01-10T04:57:56.851449Z","iopub.status.idle":"2025-01-10T04:58:00.017904Z","shell.execute_reply.started":"2025-01-10T04:57:56.851418Z","shell.execute_reply":"2025-01-10T04:58:00.016695Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchtext==0.17.0 in /usr/local/lib/python3.10/dist-packages (0.17.0)\nRequirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.10/dist-packages (2.2.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (4.66.5)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (2.32.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (1.26.4)\nRequirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0) (0.7.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.6.85)\nRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext==0.17.0) (2.2.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install datasets\n!python -m spacy download de_core_news_sm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T04:58:00.019279Z","iopub.execute_input":"2025-01-10T04:58:00.019567Z","iopub.status.idle":"2025-01-10T04:58:10.579092Z","shell.execute_reply.started":"2025-01-10T04:58:00.019546Z","shell.execute_reply":"2025-01-10T04:58:10.578049Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting de-core-news-sm==3.7.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.6)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.12.5)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.9.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (71.0.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.1)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.23.4)\nRequirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.8.30)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.8.1)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.19.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\nRequirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.16.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('de_core_news_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%%writefile main.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nimport numpy as np\nimport spacy\nimport datasets\nimport torchtext\nimport tqdm\nfrom tqdm import trange\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport time\nimport os\nimport json\n\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom filelock import FileLock\n\n\n\nSEED = 1234\nROOT = \".\"\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\n\ndef setup(rank, world_size):\n    if world_size > 1:\n        os.environ['MASTER_ADDR'] = 'localhost'\n        os.environ['MASTER_PORT'] = '12345'\n        dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    if dist.is_initialized():\n        dist.destroy_process_group()\n\ndataset = datasets.load_dataset(\"bentrevett/multi30k\")\n\ntrain_data, valid_data, test_data = (\n    dataset[\"train\"],\n    dataset[\"validation\"],\n    dataset[\"test\"],\n)\n\nen_nlp = spacy.load(\"en_core_web_sm\")\nde_nlp = spacy.load(\"de_core_news_sm\")\n\ndef tokenize_example(example, en_nlp, de_nlp, max_length, lower, sos_token, eos_token):\n    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n    de_tokens = [token.text for token in de_nlp.tokenizer(example[\"de\"])][:max_length]\n    if lower:\n        en_tokens = [token.lower() for token in en_tokens]\n        de_tokens = [token.lower() for token in de_tokens]\n    en_tokens = [sos_token] + en_tokens + [eos_token]\n    de_tokens = [sos_token] + de_tokens + [eos_token]\n    return {\"en_tokens\": en_tokens, \"de_tokens\": de_tokens}\n\nmax_length = 1_000\nlower = True\nsos_token = \"<sos>\"\neos_token = \"<eos>\"\n\nfn_kwargs = {\n    \"en_nlp\": en_nlp,\n    \"de_nlp\": de_nlp,\n    \"max_length\": max_length,\n    \"lower\": lower,\n    \"sos_token\": sos_token,\n    \"eos_token\": eos_token,\n}\n\ntrain_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\nvalid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\ntest_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n\nmin_freq = 2\nunk_token = \"<unk>\"\npad_token = \"<pad>\"\n\nspecial_tokens = [\n    unk_token,\n    pad_token,\n    sos_token,\n    eos_token,\n]\n\nen_vocab = torchtext.vocab.build_vocab_from_iterator(\n    train_data[\"en_tokens\"],\n    min_freq=min_freq,\n    specials=special_tokens,\n)\n\nde_vocab = torchtext.vocab.build_vocab_from_iterator(\n    train_data[\"de_tokens\"],\n    min_freq=min_freq,\n    specials=special_tokens,\n)\n\nassert en_vocab[unk_token] == de_vocab[unk_token]\nassert en_vocab[pad_token] == de_vocab[pad_token]\n\nunk_index = en_vocab[unk_token]\npad_index = en_vocab[pad_token]\n\nen_vocab.set_default_index(unk_index)\nde_vocab.set_default_index(unk_index)\n\ndef numericalize_example(example, en_vocab, de_vocab):\n    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n    de_ids = de_vocab.lookup_indices(example[\"de_tokens\"])\n    return {\"en_ids\": en_ids, \"de_ids\": de_ids}\n\nfn_kwargs = {\"en_vocab\": en_vocab, \"de_vocab\": de_vocab}\n\ntrain_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\nvalid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\ntest_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n\ndata_type = \"torch\"\nformat_columns = [\"en_ids\", \"de_ids\"]\n\ntrain_data = train_data.with_format(\n    type=data_type, columns=format_columns, output_all_columns=True\n)\n\nvalid_data = valid_data.with_format(\n    type=data_type,\n    columns=format_columns,\n    output_all_columns=True,\n)\n\ntest_data = test_data.with_format(\n    type=data_type,\n    columns=format_columns,\n    output_all_columns=True,\n)\n\ndef get_collate_fn(pad_index):\n    def collate_fn(batch):\n        batch_en_ids = [example[\"en_ids\"] for example in batch]\n        batch_de_ids = [example[\"de_ids\"] for example in batch]\n        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n        batch_de_ids = nn.utils.rnn.pad_sequence(batch_de_ids, padding_value=pad_index)\n        batch = {\n            \"en_ids\": batch_en_ids,\n            \"de_ids\": batch_de_ids,\n        }\n        return batch\n\n    return collate_fn\n\n\n\nbatch_size = 128\n\n\nclass Encoder(nn.Module):\n    def __init__(\n        self, input_dim, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, embedding_dim)\n        self.rnn = nn.GRU(embedding_dim, encoder_hidden_dim, bidirectional=True)\n        self.fc = nn.Linear(encoder_hidden_dim * 2, decoder_hidden_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, hidden = self.rnn(embedded)\n        hidden = torch.tanh(\n            self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n        )\n        return outputs, hidden\n\nclass Attention(nn.Module):\n    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n        super().__init__()\n        self.attn_fc = nn.Linear(\n            (encoder_hidden_dim * 2) + decoder_hidden_dim, decoder_hidden_dim\n        )\n        self.v_fc = nn.Linear(decoder_hidden_dim, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs):\n        batch_size = encoder_outputs.shape[1]\n        src_length = encoder_outputs.shape[0]\n        hidden = hidden.unsqueeze(1).repeat(1, src_length, 1)\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        energy = torch.tanh(self.attn_fc(torch.cat((hidden, encoder_outputs), dim=2)))\n        attention = self.v_fc(energy).squeeze(2)\n        return torch.softmax(attention, dim=1)\n\nclass Decoder(nn.Module):\n    def __init__(\n        self,\n        output_dim,\n        embedding_dim,\n        encoder_hidden_dim,\n        decoder_hidden_dim,\n        dropout,\n        attention,\n    ):\n        super().__init__()\n        self.output_dim = output_dim\n        self.attention = attention\n        self.embedding = nn.Embedding(output_dim, embedding_dim)\n        self.rnn = nn.GRU((encoder_hidden_dim * 2) + embedding_dim, decoder_hidden_dim)\n        self.fc_out = nn.Linear(\n            (encoder_hidden_dim * 2) + decoder_hidden_dim + embedding_dim, output_dim\n        )\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input, hidden, encoder_outputs):\n        input = input.unsqueeze(0)\n        embedded = self.dropout(self.embedding(input))\n        a = self.attention(hidden, encoder_outputs)\n        a = a.unsqueeze(1)\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        weighted = torch.bmm(a, encoder_outputs)\n        weighted = weighted.permute(1, 0, 2)\n        rnn_input = torch.cat((embedded, weighted), dim=2)\n        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n        assert (output == hidden).all()\n        embedded = embedded.squeeze(0)\n        output = output.squeeze(0)\n        weighted = weighted.squeeze(0)\n        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n        return prediction, hidden.squeeze(0), a.squeeze(1)\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio):\n        batch_size = src.shape[1]\n        trg_length = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n        encoder_outputs, hidden = self.encoder(src)\n        input = trg[0, :]\n        for t in range(1, trg_length):\n            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = trg[t] if teacher_force else top1\n        return outputs\n\ndef create_model(de_vocab,en_vocab):\n  input_dim = len(de_vocab)\n  output_dim = len(en_vocab)\n  encoder_embedding_dim = 256\n  decoder_embedding_dim = 256\n  encoder_hidden_dim = 512\n  decoder_hidden_dim = 512\n  encoder_dropout = 0.5\n  decoder_dropout = 0.5\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n  attention = Attention(encoder_hidden_dim, decoder_hidden_dim)\n\n  encoder = Encoder(\n      input_dim,\n      encoder_embedding_dim,\n      encoder_hidden_dim,\n      decoder_hidden_dim,\n      encoder_dropout,\n  )\n\n  decoder = Decoder(\n      output_dim,\n      decoder_embedding_dim,\n      encoder_hidden_dim,\n      decoder_hidden_dim,\n      decoder_dropout,\n      attention,\n  )\n\n  model = Seq2Seq(encoder, decoder, device).to(device)\n  def init_weights(m):\n    for name, param in m.named_parameters():\n        if \"weight\" in name:\n            nn.init.normal_(param.data, mean=0, std=0.01)\n        else:\n            nn.init.constant_(param.data, 0)\n\n\n  model.apply(init_weights)\n  return model\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\ndef create_dataloader(rank, world_size, batch_size, root=ROOT):\n    train_sampler = DistributedSampler(train_data, num_replicas=world_size, rank=rank, shuffle=True) if world_size > 1 else None\n    val_sampler = DistributedSampler(valid_data, num_replicas=world_size, rank=rank, shuffle=False) if world_size > 1 else None\n    test_sampler = DistributedSampler(test_data, num_replicas=world_size, rank=rank, shuffle=False) if world_size > 1 else None\n\n    train_loader = torch.utils.data.DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        collate_fn=get_collate_fn(pad_index),\n        shuffle=(train_sampler is None),\n        sampler=train_sampler,\n    )\n    val_loader = torch.utils.data.DataLoader(\n        dataset=valid_data,\n        batch_size=batch_size,\n         collate_fn=get_collate_fn(pad_index),\n         shuffle=(val_sampler is None),\n        sampler=val_sampler,\n    )\n    test_loader = torch.utils.data.DataLoader(\n        dataset=test_data,\n        batch_size=batch_size,\n         collate_fn=get_collate_fn(pad_index),\n         shuffle=(test_sampler is None),\n        sampler=test_sampler,\n    )\n    return train_loader, val_loader, test_loader\n\nRESULTS_FILE = f\"{ROOT}/project4-2gpus.json\"\n\ndef log_results(scenario, results, rank):\n    lock = FileLock(f\"{RESULTS_FILE}.lock\")\n    with lock:\n        if os.path.exists(RESULTS_FILE):\n            with open(RESULTS_FILE, 'r') as f:\n                try:\n                    all_results = json.load(f)\n                except json.JSONDecodeError:\n                    all_results = {}\n        else:\n            all_results = {}\n\n        results['rank'] = rank\n        all_results[scenario] = results\n        with open(RESULTS_FILE, 'w') as f:\n            json.dump(all_results, f, indent=4)\n\ndef calculate_accuracy(y_pred, y):\n    top_pred = y_pred.argmax(1, keepdim=True)\n    correct = top_pred.eq(y.view_as(top_pred)).sum()\n    acc = correct.float() / y.shape[0]\n    return acc\n\ndef train(model, iterator, optimizer, criterion, clip, teacher_forcing_ratio, device, batch_size):\n    model.train()\n    epoch_loss = 0\n    total_samples = 0\n    start_time = time.monotonic()\n\n    for i, batch in enumerate(iterator):\n        src = batch[\"de_ids\"].to(device)\n        trg = batch[\"en_ids\"].to(device)\n        total_samples += trg.shape[1]\n        optimizer.zero_grad()\n        output = model(src, trg, teacher_forcing_ratio)\n        output_dim = output.shape[-1]\n        output = output[1:].view(-1, output_dim)\n        trg = trg[1:].view(-1)\n        output = output.to(device)\n        trg = trg.to(device)\n        loss = criterion(output, trg)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    end_time = time.monotonic()\n    epoch_time = end_time - start_time\n    samples_per_second = total_samples / epoch_time\n\n    return epoch_loss / len(iterator), samples_per_second\n\n\ndef evaluate(model, iterator, criterion, device, batch_size):\n    model.eval()\n    epoch_loss = 0\n    total_samples = 0\n    start_time = time.monotonic()\n\n    with torch.no_grad():\n        for i, batch in enumerate(iterator):\n            src = batch[\"de_ids\"].to(device)\n            trg = batch[\"en_ids\"].to(device)\n            total_samples += trg.shape[1]\n            output = model(src, trg, 0)\n            output_dim = output.shape[-1]\n            output = output[1:].view(-1, output_dim)\n            trg = trg[1:].view(-1)\n            output = output.to(device)\n            trg = trg.to(device)\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n    end_time = time.monotonic()\n    epoch_time = end_time - start_time\n    samples_per_second = total_samples / epoch_time\n\n    return epoch_loss / len(iterator), samples_per_second\n\ndef main_train(rank, world_size, root=ROOT, num_epochs=3, batch_size=32):\n    if world_size > 1:\n        setup(rank, world_size)\n        print(f\"Process {rank} initialized.\")\n\n    train_dataloader, val_dataloader, test_dataloader = create_dataloader(rank, world_size, batch_size=batch_size, root=root)\n    model = create_model(de_vocab,en_vocab).to(rank)\n    ddp_model = DDP(model, device_ids=[rank]) if world_size > 1 else model\n    LR = 5e-4\n    clip = 1.0\n    teacher_forcing_ratio = 0.5\n    criterion = nn.CrossEntropyLoss(ignore_index=pad_index).to(rank)\n    optimizer = optim.Adam(ddp_model.parameters(), lr=LR)\n    best_valid_loss = float('inf')\n    training_times = []\n    train_losses = []\n    validation_times = []\n    validation_losses = []\n    epoch_times = []\n    train_throughputs = []\n    validation_throughputs = []\n    test_throughputs = []\n    test_losses = []\n\n    for epoch in trange(num_epochs, desc=\"Epochs\", leave = False):\n        start_epoch_time = time.monotonic()\n\n        start_time = time.monotonic()\n        train_loss, train_throughput = train(ddp_model, train_dataloader, optimizer, criterion, clip, teacher_forcing_ratio, rank, batch_size)\n        train_time = time.monotonic() - start_time\n        training_times.append(train_time)\n        train_losses.append(train_loss)\n        train_throughputs.append(train_throughput)\n\n        start_time = time.monotonic()\n        valid_loss,  valid_throughput = evaluate(ddp_model, val_dataloader, criterion, rank, batch_size)\n        val_time = time.monotonic() - start_time\n        validation_times.append(val_time)\n        validation_losses.append(valid_loss)\n        validation_throughputs.append(valid_throughput)\n\n        if valid_loss < best_valid_loss and rank==0:\n            best_valid_loss = valid_loss\n            torch.save(ddp_model.state_dict(), f'{root}tut4-model.pt')\n\n        start_time = time.monotonic()\n        test_loss, test_throughput = evaluate(ddp_model, test_dataloader, criterion, rank, batch_size)\n        test_time = time.monotonic() - start_time\n        test_losses.append(test_loss)\n        test_throughputs.append(test_throughput)\n\n\n        end_time = time.monotonic()\n        epoch_mins, epoch_secs = epoch_time(start_epoch_time, end_time)\n        epoch_times.append(end_time - start_epoch_time)\n\n        print(f'--------------|     On process {rank}      |----------------')\n        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n        print(f'\\tTrain Loss: {train_loss:.3f}')\n        print(f'\\t Val. Loss: {valid_loss:.3f}')\n        print(f'\\t Test Loss: {test_loss:.3f}')\n\n\n    results = {\n        \"world_size\": world_size,\n        \"training_times\": training_times,\n        \"train_losses\": train_losses,\n        \"validation_times\": validation_times,\n        \"validation_losses\": validation_losses,\n         \"test_losses\": test_losses,\n        \"epoch_times\": epoch_times,\n        \"train_throughputs\": train_throughputs,\n        \"validation_throughputs\": validation_throughputs,\n        \"test_throughputs\": test_throughputs,\n    }\n\n    model_name = \"seq2seq\"\n    scenario = f\"{model_name}_{world_size}_GPUs_rank_{rank}\"  # Create unique scenario name\n    log_results(scenario, results, rank)\n\n    if world_size > 1:\n        dist.barrier()\n        cleanup()\n    print(f'Process {rank} finished training.')\n\nif __name__ == \"__main__\":\n    def main():\n        world_size = torch.cuda.device_count()\n        print(f'Total number of devices detected: {world_size}')\n\n        if world_size > 0:\n           # world_size-=1  #delete to use 2 GPUs , Keep to use 1 GPu\n            if world_size > 1:\n                mp.spawn(\n                    main_train,\n                    args = (world_size, ROOT),\n                    nprocs = world_size,\n                    join = True\n                )\n            else:\n                main_train(rank=0, world_size=1)\n\n        else:\n            print('no GPUs found. Please make sure you have configured CUDA correctly')\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T04:58:10.580727Z","iopub.execute_input":"2025-01-10T04:58:10.580981Z","iopub.status.idle":"2025-01-10T04:58:10.588997Z","shell.execute_reply.started":"2025-01-10T04:58:10.580960Z","shell.execute_reply":"2025-01-10T04:58:10.588105Z"}},"outputs":[{"name":"stdout","text":"Overwriting main.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!python main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T04:58:10.590607Z","iopub.execute_input":"2025-01-10T04:58:10.590854Z","iopub.status.idle":"2025-01-10T05:03:04.038420Z","shell.execute_reply.started":"2025-01-10T04:58:10.590827Z","shell.execute_reply":"2025-01-10T05:03:04.037585Z"}},"outputs":[{"name":"stdout","text":"Map: 100%|█████████████████████████| 1014/1014 [00:00<00:00, 2692.77 examples/s]\nMap: 100%|█████████████████████████| 1000/1000 [00:00<00:00, 3678.72 examples/s]\nMap: 100%|█████████████████████████| 1014/1014 [00:00<00:00, 8779.39 examples/s]\nMap: 100%|█████████████████████████| 1000/1000 [00:00<00:00, 9274.18 examples/s]\nTotal number of devices detected: 2\nMap: 100%|███████████████████████| 29000/29000 [00:06<00:00, 4632.28 examples/s]\nMap: 100%|███████████████████████| 29000/29000 [00:06<00:00, 4594.35 examples/s]\nMap: 100%|█████████████████████████| 1014/1014 [00:00<00:00, 4603.32 examples/s]\nMap: 100%|█████████████████████████| 1000/1000 [00:00<00:00, 5253.50 examples/s]\nMap: 100%|█████████████████████████| 1000/1000 [00:00<00:00, 5201.37 examples/s]\nMap: 100%|███████████████████████| 29000/29000 [00:03<00:00, 8398.76 examples/s]\nMap: 100%|█████████████████████████| 1014/1014 [00:00<00:00, 4735.96 examples/s]\nMap: 100%|███████████████████████| 29000/29000 [00:03<00:00, 7805.43 examples/s]\nMap: 100%|█████████████████████████| 1000/1000 [00:00<00:00, 5132.86 examples/s]\nMap: 100%|█████████████████████████| 1000/1000 [00:00<00:00, 8884.09 examples/s]\nProcess 1 initialized.\nProcess 0 initialized.\nEpochs:   0%|                                             | 0/3 [00:00<?, ?it/s]--------------|     On process 0      |----------------\nEpoch: 01 | Epoch Time: 1m 25s\n\tTrain Loss: 4.898\n\t Val. Loss: 4.531\n\t Test Loss: 4.499\nEpochs:  33%|████████████▎                        | 1/3 [01:25<02:50, 85.12s/it]--------------|     On process 1      |----------------\nEpoch: 01 | Epoch Time: 1m 25s\n\tTrain Loss: 4.910\n\t Val. Loss: 4.769\n\t Test Loss: 4.763\nEpochs:  33%|████████████▎                        | 1/3 [01:25<02:50, 85.34s/it]--------------|     On process 0      |----------------\nEpoch: 02 | Epoch Time: 1m 26s\n\tTrain Loss: 4.024\n\t Val. Loss: 4.079\n\t Test Loss: 4.012\nEpochs:  67%|████████████████████████▋            | 2/3 [02:51<01:25, 85.88s/it]--------------|     On process 1      |----------------\nEpoch: 02 | Epoch Time: 1m 26s\n\tTrain Loss: 4.030\n\t Val. Loss: 4.381\n\t Test Loss: 4.371\nEpochs:  67%|████████████████████████▋            | 2/3 [02:51<01:25, 85.97s/it]--------------|     On process 0      |----------------\nEpoch: 03 | Epoch Time: 1m 26s\n\tTrain Loss: 3.425\n\t Val. Loss: 3.559\n\t Test Loss: 3.519\n--------------|     On process 1      |----------------                         \nEpoch: 03 | Epoch Time: 1m 26s\n\tTrain Loss: 3.433\n\t Val. Loss: 3.919\n\t Test Loss: 3.904\nProcess 1 finished training.                                                    \nProcess 0 finished training.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}